# Strimzi Cluster Helm Chart

{{ template "chart.badgesSection" . }}

{{ template "chart.description" . }}

{{ template "chart.sourcesSection" . }}

{{ template "chart.requirementsSection" . }}

## Usage example

```bash
helm repo add stevenjdh https://StevenJDH.github.io/helm-charts
helm repo update
helm upgrade --install my-{{ template "chart.name" . }} stevenjdh/{{ template "chart.name" . }} --version {{ template "chart.version" . }} \
    --set strimzi-kafka-operator.enabled=true \
    --set strimzi-drain-cleaner.enabled=true \
    --set strimzi-drain-cleaner.certManager.create=false \
    --set-file strimzi-drain-cleaner.secret.tls_crt=tls.crt.base64 \
    --set-file strimzi-drain-cleaner.secret.tls_key=tls.key.base64 \
    --set-file strimzi-drain-cleaner.secret.ca_bundle=ca.crt.base64 \
    --set kafka.rackTopology.enabled=false \
    --namespace example \
    --create-namespace \
    --atomic
```

> [!TIP]
> To test Drain Cleaner, run the command `kubectl drain <node-name> --delete-emptydir-data --ignore-daemonsets --timeout=6000s --force` against a node with a broker, which will fail the first time because the strimzi cluster operator will take over for relocating those workloads. Then, rerun the command again after a few minutes, and it will work this time. For more info, see [Using the Strimzi Drain Cleaner](https://github.com/strimzi/drain-cleaner?tab=readme-ov-file#see-it-in-action).

### Create Drain Cleaner certificate chain

The following shows how to create the needed TLS certificates if Drain Cleaner will be installed with `strimzi-drain-cleaner.certManager.create` set to `false` as per above example. OpenSSL CLI v1.1.1 or newer is required.

```bash
# 1. Set the namespace used by Drain Cleaner.
namespace=strimzi-drain-cleaner

# 2. Create CA certificate and key.
openssl req -x509 -sha256 -newkey rsa:4096 -keyout ca.key -out ca.crt -days 11688 -noenc \
    -subj "/CN=Strimzi Drain Cleaner Root CA/O=StevenJDH" \
    -addext "basicConstraints=critical,CA:TRUE,pathlen:1" \
    -addext "keyUsage=critical,keyCertSign,cRLSign" \
    -addext "subjectKeyIdentifier=hash"

# 3. Create certificate signing request (*.csr) and private key.
openssl req -new -newkey rsa:4096 -keyout tls.key -out tls.csr -noenc \
    -subj "/CN=strimzi-drain-cleaner.$namespace.svc/O=StevenJDH" \
    -addext "basicConstraints=critical,CA:FALSE" \
    -addext "extendedKeyUsage=serverAuth" \
    -addext "keyUsage=critical,digitalSignature,keyEncipherment,keyAgreement" \
    -addext "subjectAltName=DNS:strimzi-drain-cleaner.$namespace.svc" \
    -addext "subjectKeyIdentifier=hash"

# 4. Created CA signed server certificate from CSR.
openssl x509 -req -sha256 -days 11688 -in tls.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out tls.crt \
    -copy_extensions copy

# 5. Base64 encode the files as expected by the Drain Cleaner.
base64 -w0 tls.crt > tls.crt.base64
base64 -w0 tls.key > tls.key.base64
base64 -w0 ca.crt > ca.crt.base64
```

## Monitoring with Prometheus and Grafana

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 67.11.0
    -f prometheus-values.yaml \ # Check below for one of the options to use for this file.
    --namespace monitoring \
    --create-namespace \
    --atomic
```

For alerts, apply the [Prometheus Alert Rules](https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/refs/tags/0.45.0/examples/metrics/prometheus-install/prometheus-rules.yaml) to the monitoring namespace, and add to it the label `release: kube-prometheus-stack` so it will be imported automatically.

### Option 1 - Using PodMonitor (Recommended)
This recommended approach will automatically detect and directly collect metrics from Strimzi related pods.

**prometheus-values.yaml**

```yaml
grafana:
  defaultDashboardsEnabled: false
  adminPassword: admin

prometheus:
  prometheusSpec:
    # Disabling this adds better support for third-party PodMonitor resource detection
    # in its namespace without having to deal with label filtering or compromising the
    # default discovery. Otherwise, the 'release: kube-prometheus-stack' label needs
    # to be present in CRD resources like PodMonitor. To get the release name if
    # the chart is already installed, use 'helm list -n monitoring'.
    podMonitorSelectorNilUsesHelmValues: true

    # PodMonitors to be selected for target discovery. If {}, select all PodMonitors.
    podMonitorSelector: {}
      # matchLabels:
      #   prometheus: main
      
    # If {}, select own namespace (e.g., monitoring). Namespaces matching labels to
    # be selected for PodMonitor discovery. Useful for when wanting to keep
    # resources together with app instead of grouped together in monitoring namespace.
    podMonitorNamespaceSelector: {}
      # matchLabels:
      #   monitoring: prometheus
```

After the kube-prometheus-stack chart has been deployed or updated with the config above, set `podMonitor.create` to `true` in the strimzi-cluster chart.

### Option 2 - Using Headless Services
This approach is more for compatibility reasons. For example, when using CRDs is not an option. If using [prometheus-community/prometheus](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus) instead of the [prometheus-community/kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack), `prometheus.prometheusSpec.additionalScrapeConfigs` becomes `extraScrapeConfigs`, and the `grafana` section is dropped.

**prometheus-values.yaml**

```yaml
grafana:
  defaultDashboardsEnabled: false
  adminPassword: admin

prometheus:
  prometheusSpec:
    additionalScrapeConfigs:
    - job_name: strimzi-brokers-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-broker-metrics-headless.strimzi.svc.cluster.local
      relabelings:
        - separator: ;
          regex: __meta_kubernetes_pod_label_(strimzi_io_.+)
          replacement: $1
          action: labelmap
        - sourceLabels: [__meta_kubernetes_namespace]
          separator: ;
          regex: (.*)
          targetLabel: namespace
          replacement: $1
          action: replace
        - sourceLabels: [__meta_kubernetes_pod_name]
          separator: ;
          regex: (.*)
          targetLabel: kubernetes_pod_name
          replacement: $1
          action: replace
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          separator: ;
          regex: (.*)
          targetLabel: node_name
          replacement: $1
          action: replace
        - sourceLabels: [__meta_kubernetes_pod_host_ip]
          separator: ;
          regex: (.*)
          targetLabel: node_ip
          replacement: $1
          action: replace
    - job_name: strimzi-kraft-controllers-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-kraft-controller-metrics-headless.strimzi.svc.cluster.local
    - job_name: strimzi-cluster-operator-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-cluster-operator-metrics-headless.strimzi.svc.cluster.local
    - job_name: strimzi-entity-operator-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-entity-operator-metrics-headless.strimzi.svc.cluster.local
    - job_name: strimzi-cruise-control-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-cruise-control-metrics-headless.strimzi.svc.cluster.local
    - job_name: strimzi-kafka-exporter-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-kafka-exporter-metrics-headless.strimzi.svc.cluster.local
```

> [!NOTE]  
> The above config assumes that the strimzi-cluster chart is deployed to the `strimzi` namespace. If not, then update to match. Also, the relabeling section hasn't been applied to every job for conciseness. To match Option 1's relabeling, apply to each job except for the two operators.

After the kube-prometheus-stack chart has been deployed or updated with the config above, set `scrapeConfigHeadlessServices.create` to `true` in the strimzi-cluster chart.

## Load testing
Below is a simple script to perform a load test on a cluster using a Grafana managed tool called K6 that is making use of a Kafka extension. If it hasn't been done already, set `testResources.create` to `true` in the chart to create the test resources, and set `kafka.authorization` to `null` to disable ACL checks. Check if the setup is ready by running the following commands:

**Window 1 - Producer:**

```bash
kubectl run kafka-producer --image=quay.io/strimzi/kafka:0.45.0-kafka-3.9.0 -it --rm --restart=Never -n strimzi \
  -- bin/kafka-console-producer.sh --bootstrap-server strimzi-cluster-kafka-bootstrap:9092 --topic test-topic

# Or

kubectl exec strimzi-cluster-broker-0 -it -c kafka -n strimzi \
  -- bin/kafka-console-producer.sh --bootstrap-server strimzi-cluster-kafka-bootstrap:9092 --topic test-topic
```

Type a message when you see the `>` character.

**Window 2 - Consumer:**

```bash
kubectl run kafka-consumer --image=quay.io/strimzi/kafka:0.45.0-kafka-3.9.0 -it --rm --restart=Never -n strimzi \
  -- bin/kafka-console-consumer.sh --bootstrap-server strimzi-cluster-kafka-bootstrap:9092 \
  --topic test-topic --group test-consumer-group --from-beginning

# Or

kubectl exec strimzi-cluster-broker-0 -it -c kafka -n strimzi \
  -- bin/kafka-console-consumer.sh --bootstrap-server strimzi-cluster-kafka-bootstrap:9092 \
  --topic test-topic --group test-consumer-group --from-beginning
```

Produced messages should be visible now after being consumed. The commands can be exited by pressing `Ctrl+C`. To begin the load tests and stop it, use the following commands:

```bash
# Deploy load test
kubectl create -f k6-load-test.yaml
# View logs in realtime (-f)
kubectl logs -l app.kubernetes.io/name=k6-load-test --tail=100 -f -n strimzi
# Stop load test (or, alternatively scale to 0)
kubectl delete -f k6-load-test.yaml
```

When finished, the Grafana Dashboard for Kafka Exporter will have useful information about the test as well. Since the consumer window was terminated, the dashboard should show a huge lag for the consumer group. Start the consumer again, and wait for the lag to drop back down to 0. By then, there will be additional numbers to check the consumption rate per second.

**k6-load-test.yaml**

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: load-test-config
  namespace: strimzi
data:
  load-test.js: |
    import {
      Writer,
      SchemaRegistry,
      SCHEMA_TYPE_JSON,
    } from "k6/x/kafka";
    const writer = new Writer({
      brokers: [__ENV.KAFKA_URL],
      topic: __ENV.TOPIC,
    });
    const schemaRegistry = new SchemaRegistry();
    export default function () {
      writer.produce({
        messages: [
          {
            value: schemaRegistry.serialize({
              data: {
                id: 1,
                source: "k6-load-test",
                space: "strimzi",
                cluster: __ENV.CLUSTER,
                message: "Hello World!"
              },
              schemaType: SCHEMA_TYPE_JSON,
            }),
          },
        ],
      });
    }
    export function teardown(data) {
      writer.close();
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k6-load-test
  namespace: strimzi
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: k6-load-test
  template:
    metadata:
      labels:
        app.kubernetes.io/name: k6-load-test
    spec:
      containers:
        - image: mostafamoradian/xk6-kafka:latest
          name: xk6-kafka
          command:
            - "k6"
            - "run"
            - "--vus"
            - "1"
            - "--duration"
            - "60s"
            - "/tests/load-test.js"
          env:
            - name: KAFKA_URL
              value: strimzi-cluster-kafka-bootstrap:9092
            - name: CLUSTER
              value: strimzi-cluster
            - name: TOPIC
              value: test-topic
            - name: POD
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - mountPath: /tests
              name: test-script
      volumes:
        - name: test-script
          configMap:
            name: load-test-config
```

> [!TIP]
> The flags `--vus` and `--duration` represent the number of concurrent virtual users and the duration specified in a format of `s` (seconds), `m` (minutes), or `h` (hours). Adjust these values as needed.

{{ template "chart.valuesSection" . }}


{{ template "footer.signature" . }}

# Strimzi Cluster Helm Chart

{{ template "chart.badgesSection" . }}

{{ template "chart.description" . }}

{{ template "chart.sourcesSection" . }}

{{ template "chart.requirementsSection" . }}

## Usage example

```bash
helm repo add stevenjdh https://StevenJDH.github.io/helm-charts
helm repo update
helm upgrade --install my-{{ template "chart.name" . }} stevenjdh/{{ template "chart.name" . }} --version {{ template "chart.version" . }} \
    --set strimzi-kafka-operator.enabled=true \
    --set strimzi-drain-cleaner.enabled=true \
    --set strimzi-drain-cleaner.certManager.create=false \
    --set-file strimzi-drain-cleaner.secret.tls_crt=tls.crt.base64 \
    --set-file strimzi-drain-cleaner.secret.tls_key=tls.key.base64 \
    --set-file strimzi-drain-cleaner.secret.ca_bundle=ca.crt.base64 \
    --set kafka.rackTopology.enabled=false \
    --namespace example \
    --create-namespace \
    --atomic
```

> [!TIP]
> To test Drain Cleaner, run the command `kubectl drain <node-name> --delete-emptydir-data --ignore-daemonsets --timeout=6000s --force` against a node with a broker, which will fail the first time because the strimzi cluster operator will take over for relocating those workloads. Then, rerun the command again after a few minutes, and it will work this time. For more info, see [Using the Strimzi Drain Cleaner](https://github.com/strimzi/drain-cleaner?tab=readme-ov-file#see-it-in-action).

### Create Drain Cleaner certificate chain

The following shows how to create the needed TLS certificates if Drain Cleaner will be installed with `strimzi-drain-cleaner.certManager.create` set to `false` as per above example. OpenSSL CLI v1.1.1 or newer is required.

```bash
# 1. Set the namespace used by Drain Cleaner.
namespace=strimzi-drain-cleaner

# 2. Create CA certificate and key.
openssl req -x509 -sha256 -newkey rsa:4096 -keyout ca.key -out ca.crt -days 11688 -noenc \
    -subj "/CN=Strimzi Drain Cleaner Root CA/O=StevenJDH" \
    -addext "basicConstraints=critical,CA:TRUE,pathlen:1" \
    -addext "keyUsage=critical,keyCertSign,cRLSign" \
    -addext "subjectKeyIdentifier=hash"

# 3. Create certificate signing request (*.csr) and private key.
openssl req -new -newkey rsa:4096 -keyout tls.key -out tls.csr -noenc \
    -subj "/CN=strimzi-drain-cleaner.$namespace.svc/O=StevenJDH" \
    -addext "basicConstraints=critical,CA:FALSE" \
    -addext "extendedKeyUsage=serverAuth" \
    -addext "keyUsage=critical,digitalSignature,keyEncipherment,keyAgreement" \
    -addext "subjectAltName=DNS:strimzi-drain-cleaner.$namespace.svc" \
    -addext "subjectKeyIdentifier=hash"

# 4. Created CA signed server certificate from CSR.
openssl x509 -req -sha256 -days 11688 -in tls.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out tls.crt \
    -copy_extensions copy

# 5. Base64 encode the files as expected by the Drain Cleaner.
base64 -w0 tls.crt > tls.crt.base64
base64 -w0 tls.key > tls.key.base64
base64 -w0 ca.crt > ca.crt.base64
```

## Monitoring with Prometheus and Grafana

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 68.1.0
    -f prometheus-values.yaml \ # Check below for one of the options to use for this file.
    --namespace monitoring \
    --create-namespace \
    --atomic
```

For alerts, apply the [Prometheus Alert Rules](https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/refs/tags/0.45.0/examples/metrics/prometheus-install/prometheus-rules.yaml) to the monitoring namespace, and add to it the label `release: kube-prometheus-stack` so it will be imported automatically.

### Option 1 - Using PodMonitor (Recommended)
This recommended approach will automatically detect and directly collect metrics from Strimzi related pods.

**prometheus-values.yaml**

```yaml
grafana:
  defaultDashboardsEnabled: false
  adminPassword: admin

prometheus:
  prometheusSpec:
    # Disabling this adds better support for third-party PodMonitor resource detection
    # in its namespace without having to deal with label filtering or compromising the
    # default discovery. Otherwise, the 'release: kube-prometheus-stack' label needs
    # to be present in CRD resources like PodMonitor. To get the release name if
    # the chart is already installed, use 'helm list -n monitoring'.
    podMonitorSelectorNilUsesHelmValues: true

    # PodMonitors to be selected for target discovery. If {}, select all PodMonitors.
    podMonitorSelector: {}
      # matchLabels:
      #   prometheus: main
      
    # If {}, select own namespace (e.g., monitoring). Namespaces matching labels to
    # be selected for PodMonitor discovery. Useful for when wanting to keep
    # resources together with app instead of grouped together in monitoring namespace.
    podMonitorNamespaceSelector: {}
      # matchLabels:
      #   monitoring: prometheus
```

After the kube-prometheus-stack chart has been deployed or updated with the config above, set `podMonitor.create` to `true` in the strimzi-cluster chart.

### Option 2 - Using Headless Services
This approach is more for compatibility reasons. For example, when using CRDs is not an option. If using [prometheus-community/prometheus](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus) instead of the [prometheus-community/kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack), `prometheus.prometheusSpec.additionalScrapeConfigs` becomes `extraScrapeConfigs`, and the `grafana` section is dropped.

**prometheus-values.yaml**

```yaml
grafana:
  defaultDashboardsEnabled: false
  adminPassword: admin

prometheus:
  prometheusSpec:
    additionalScrapeConfigs:
    - job_name: strimzi-brokers-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-broker-metrics-headless.strimzi.svc.cluster.local
      relabelings:
        - separator: ;
          regex: __meta_kubernetes_pod_label_(strimzi_io_.+)
          replacement: $1
          action: labelmap
        - sourceLabels: [__meta_kubernetes_namespace]
          separator: ;
          regex: (.*)
          targetLabel: namespace
          replacement: $1
          action: replace
        - sourceLabels: [__meta_kubernetes_pod_name]
          separator: ;
          regex: (.*)
          targetLabel: kubernetes_pod_name
          replacement: $1
          action: replace
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          separator: ;
          regex: (.*)
          targetLabel: node_name
          replacement: $1
          action: replace
        - sourceLabels: [__meta_kubernetes_pod_host_ip]
          separator: ;
          regex: (.*)
          targetLabel: node_ip
          replacement: $1
          action: replace
    - job_name: strimzi-kraft-controllers-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-kraft-controller-metrics-headless.strimzi.svc.cluster.local
    - job_name: strimzi-cluster-operator-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-cluster-operator-metrics-headless.strimzi.svc.cluster.local
    - job_name: strimzi-entity-operator-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-entity-operator-metrics-headless.strimzi.svc.cluster.local
    - job_name: strimzi-cruise-control-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-cruise-control-metrics-headless.strimzi.svc.cluster.local
    - job_name: strimzi-kafka-exporter-metrics
      scrape_interval: 5s
      metrics_path: /metrics
      dns_sd_configs:
      - names:
        - strimzi-kafka-exporter-metrics-headless.strimzi.svc.cluster.local
```

> [!NOTE]  
> The above config assumes that the strimzi-cluster chart is deployed to the `strimzi` namespace. If not, then update to match. Also, the relabeling section hasn't been applied to every job for conciseness. To match Option 1's relabeling, apply to each job except for the two operators.

After the kube-prometheus-stack chart has been deployed or updated with the config above, set `scrapeConfigHeadlessServices.create` to `true` in the strimzi-cluster chart.

## Load testing
Below is a simple script to perform a load test on a cluster using a Grafana managed tool called K6 that is making use of a [Kafka extension](https://github.com/mostafa/xk6-kafka). If it hasn't been done already, set `testResources.create` to `true` in the chart to create the included test user and topic.

To start load testing using the script below, run the following commands:

```bash
# Deploy load test
kubectl create -f k6-load-test.yaml
# View logs in realtime (-f)
kubectl logs -l app.kubernetes.io/name=k6-load-test --tail=100 -f -n strimzi
# Delete load test after reviewing summary in logs
kubectl delete -f k6-load-test.yaml
```

When finished, apart from the test result summary, the Grafana Dashboard for Kafka Exporter will have useful information about the test as well.

**k6-load-test.yaml**

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: load-test-config
  namespace: strimzi
data:
  load-test.js: |
    import {
      Writer,
      SchemaRegistry,
      SCHEMA_TYPE_STRING,
      SCHEMA_TYPE_JSON,
      TLS_1_2
    } from "k6/x/kafka";

    // Reference: https://k6.io/docs/using-k6/k6-options/
    export const options = {
      thresholds: {
        kafka_writer_error_count: ["count == 0"],
        kafka_reader_error_count: ["count == 0"],
      },
      scenarios: {
        // Using environment variables because CLI flags
        // apply only to the default scenario.
        // Reference: https://community.grafana.com/t/harness-docker-k6-error-function-default-not-found-in-exports/98961
        load_test: {
          exec: "load_test",
          executor: "constant-vus",
          vus: __ENV.VUS,
          duration: __ENV.DURATION,
          gracefulStop: __ENV.GRACEFUL_STOP,
        },
      },
    };

    const writer = new Writer({
      brokers: [__ENV.BOOTSTRAP_URL],
      topic: __ENV.TOPIC,
      tls: {
        enableTls: true,
        insecureSkipTLSVerify: false,
        minVersion: TLS_1_2,
        clientCertPem: __ENV.CERT_PATH,
        clientKeyPem: __ENV.KEY_PATH,
        serverCaPem: __ENV.CA_PATH,
      },
    });

    const schemaRegistry = new SchemaRegistry();

    export function load_test() {
      writer.produce({
        messages: [
          {
            key: schemaRegistry.serialize({
              data: "a-key",
              schemaType: SCHEMA_TYPE_STRING,
            }),
            value: schemaRegistry.serialize({
              data: {
                id: 1,
                source: __ENV.POD,
                namespace: __ENV.NAMESPACE,
                cluster: __ENV.CLUSTER,
                message: "Hello World!"
              },
              schemaType: SCHEMA_TYPE_JSON,
            }),
            headers: {
              foo: "bar",
            },
            time: new Date(), // Converts to timestamp automatically.
          },
        ],
      });
    }

    export function teardown(data) {
      if (writer) writer.close();
    }
---
apiVersion: v1
kind: Pod
metadata:
  name: k6-load-test
  labels:
    app.kubernetes.io/name: k6-load-test
  namespace: strimzi
spec:
  restartPolicy: Never
  containers:
    - image: mostafamoradian/xk6-kafka:latest
      name: xk6-kafka
      command: ["k6", "run", "/tests/load-test.js"]
      env:
        - name: BOOTSTRAP_URL
          value: strimzi-cluster-kafka-bootstrap:9094
        - name: CLUSTER
          value: strimzi-cluster
        - name: TOPIC
          value: test-topic
        - name: CONSUMER_GROUP
          value: test-consumer-group
        - name: CERT_PATH
          value: "/client/user.crt"
        - name: KEY_PATH
          value: "/client/user.key"
        - name: CA_PATH
          value: "/server/ca.crt"
        - name: POD
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: VUS
          value: "1"
        - name: DURATION
          value: "10s"
        - name: GRACEFUL_STOP
          value: "30s"
      volumeMounts:
        - name: client-volume
          mountPath: /client
          readOnly: true
        - name: server-volume
          mountPath: /server
          readOnly: true
        - mountPath: /tests
          name: script-volume
          readOnly: true
  volumes:
    - name: client-volume
      secret:
        secretName: test-user
    - name: server-volume
      secret:
        secretName: strimzi-cluster-cluster-ca-cert
    - name: script-volume
      configMap:
        name: load-test-config
```

> [!TIP]
> The environment variables `VUS` and `DURATION` represent the number of concurrent virtual users and the duration specified in a format of `s` (seconds), `m` (minutes), or `h` (hours). Adjust these values as needed. Use these instead of the CLI flags `--vus` and `--duration` as those apply only to the default scenario and will throw an error because it's not used.

{{ template "chart.valuesSection" . }}


{{ template "footer.signature" . }}
